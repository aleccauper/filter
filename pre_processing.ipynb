{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training dataset from flat pT PixelAV datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "threshold = 0.1\n",
    "sensor_geom = '100x25'\n",
    "train_dataset_name = 'dataset_5s' # for train datasets\n",
    "test_dataset_name = 'dataset_4s' # for location of test (physical pT) datasets\n",
    "dataset_savedir = 'dataset_5s' # for save loc of final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirtrain = '/location/of/parquets/smartpixels/'+train_dataset_name+'/'+train_dataset_name+'_'+sensor_geom+'_parquets/unflipped/'\n",
    "# /location/of/parquets/smartpixels/dataset_2s/dataset_2s_50x12P5_parquets/unflipped\n",
    "dftrain = pd.read_parquet(dirtrain+'labels_d16401.parquet')\n",
    "print(dftrain.head())\n",
    "print(dftrain.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabels = []\n",
    "trainrecons = []\n",
    "\n",
    "iter=0\n",
    "suffix = 16400\n",
    "for filepath in glob.iglob(dirtrain+'*.parquet'):\n",
    "    iter+=1\n",
    "print(iter,\" files present in directory.\")\n",
    "for i in range(int(iter/3)):\n",
    "        trainlabels.append(pd.read_parquet(dirtrain+'labels_d'+str(suffix+i+1)+'.parquet'))\n",
    "        trainrecons.append(pd.read_parquet(dirtrain+'recon2D_d'+str(suffix+i+1)+'.parquet'))\n",
    "trainlabels_csv = pd.concat(trainlabels, ignore_index=True)\n",
    "trainrecons_csv = pd.concat(trainrecons, ignore_index=True)\n",
    "\n",
    "iter_0, iter_1, iter_2 = 0, 0, 0\n",
    "iter_rem = 0\n",
    "for iter, row in trainlabels_csv.iterrows():\n",
    "    if(abs(row['pt'])>threshold):\n",
    "        iter_0+=1\n",
    "    elif(-1*threshold<=row['pt']<0):\n",
    "        iter_1+=1\n",
    "    elif(0<row['pt']<=threshold):\n",
    "        iter_2+=1\n",
    "    else:\n",
    "        iter_rem+=1\n",
    "print(\"iter_0: \",iter_0)\n",
    "print(\"iter_1: \",iter_1)\n",
    "print(\"iter_2: \",iter_2)\n",
    "print(\"iter_rem: \",iter_rem)\n",
    "\n",
    "plt.hist(trainlabels_csv['pt'], bins=100)\n",
    "plt.title('pT of all events')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(trainlabels_csv[abs(trainlabels_csv['pt'])>threshold]['pt'], bins=100)\n",
    "plt.title('pT of Class 0 events')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(trainlabels_csv[(0<=trainlabels_csv['pt'])&(trainlabels_csv['pt']<=threshold)]['pt'], bins=50)\n",
    "plt.hist(trainlabels_csv[(-1*threshold<=trainlabels_csv['pt'])& (trainlabels_csv['pt']<0)]['pt'], bins=50)\n",
    "plt.title('pT of Class 1+2 events')\n",
    "plt.show()\n",
    "\n",
    "number_of_events = (min(iter_1, iter_2)//1000)*1000\n",
    "if(number_of_events*2>iter_0):\n",
    "    number_of_events = (iter_0//1000)*1000/2\n",
    "number_of_events = int(number_of_events)\n",
    "print(\"Number of events: \",number_of_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "def sumRow(X):\n",
    "    sum1 = 0\n",
    "    sumList = []\n",
    "    for i in X:\n",
    "        sum1 = np.sum(i,axis=0)\n",
    "        sumList.append(sum1)\n",
    "        b = np.array(sumList)\n",
    "    return b\n",
    "trainlist1, trainlist2 = [], []\n",
    "\n",
    "for (index1, row1), (index2, row2) in zip(trainrecons_csv.iterrows(), trainlabels_csv.iterrows()):\n",
    "    rowSum = 0.0\n",
    "    X = row1.values\n",
    "    X = np.reshape(X,(13,21))\n",
    "    rowSum = sumRow(X)\n",
    "    trainlist1.append(rowSum)\n",
    "    cls = -1\n",
    "    if(abs(row2['pt'])>threshold):\n",
    "        cls=0\n",
    "    elif(-1*threshold<=row2['pt']<0):\n",
    "        cls=1\n",
    "    elif(0<=row2['pt']<=threshold):\n",
    "        cls=2\n",
    "    trainlist2.append([row2['y-local'], cls, row2['pt']])\n",
    "traindf_all = pd.concat([pd.DataFrame(trainlist1), pd.DataFrame(trainlist2 , columns=['y-local', 'cls', 'pt'])], axis=1)\n",
    "print(traindf_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalsize = number_of_events\n",
    "random_seed0 = 10#11\n",
    "random_seed1 = 13#14\n",
    "random_seed2 = 19#20\n",
    "\n",
    "traindf_all = traindf_all.sample(frac=1, random_state=random_seed0).reset_index(drop=True)\n",
    "# traindf_all.to_csv(dataset_savedir+'/'+'/FullTrainData_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv', index=False)\n",
    "traindfcls0 = traindf_all.loc[traindf_all['cls']==0]\n",
    "traindfcls1 = traindf_all.loc[traindf_all['cls']==1]\n",
    "traindfcls2 = traindf_all.loc[traindf_all['cls']==2]\n",
    "print(traindfcls0.shape)\n",
    "print(traindfcls1.shape)\n",
    "print(traindfcls2.shape)\n",
    "print(traindfcls2.head())\n",
    "traindfcls0 = traindfcls0.iloc[:2*totalsize]\n",
    "traindfcls1 = traindfcls1.iloc[:totalsize]\n",
    "traindfcls2 = traindfcls2.iloc[:totalsize]\n",
    "print(traindfcls2.head())\n",
    "\n",
    "traincls0 = traindfcls0.sample(frac = 1, random_state=random_seed1)\n",
    "traincls1 = traindfcls1.sample(frac = 1, random_state=random_seed1)\n",
    "traincls2 = traindfcls2.sample(frac = 1, random_state=random_seed1)\n",
    "train = pd.concat([traincls0, traincls1, traincls2], axis=0)\n",
    "\n",
    "train = train.sample(frac=1, random_state=random_seed2)\n",
    "\n",
    "print(traincls0.shape)\n",
    "print(traincls1.shape)\n",
    "print(traincls2.shape)\n",
    "print(train.shape)\n",
    "\n",
    "trainlabel = train['cls']\n",
    "trainpt = train['pt']\n",
    "train = train.drop(['cls', 'pt'], axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "print(trainlabel.shape)\n",
    "print(trainpt.shape)\n",
    "\n",
    "train.to_csv(dataset_savedir+'/FullPrecisionInputTrainSet_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv', index=False)\n",
    "trainlabel.to_csv(dataset_savedir+'/TrainSetLabel_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv', index=False)\n",
    "trainpt.to_csv(dataset_savedir+'/TrainSetPt_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test datasets from physical PixelAV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirtest = '/location/of/parquets/smartpixels/'+test_dataset_name+'/'+test_dataset_name+'_'+sensor_geom+'_parquets/unflipped/'\n",
    "# /location/of/parquets/smartpixels/dataset_2s/dataset_2s_50x12P5_parquets/unflipped\n",
    "dftest = pd.read_parquet(dirtest+'labels_d16401.parquet')\n",
    "print(dftest.head())\n",
    "print(dftest.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlabels = []\n",
    "testrecons = []\n",
    "\n",
    "iter=0\n",
    "suffix = 16400\n",
    "for filepath in glob.iglob(dirtest+'*.parquet'):\n",
    "    iter+=1\n",
    "print(iter,\" files present in directory.\")\n",
    "for i in range(int(iter/3)):\n",
    "        testlabels.append(pd.read_parquet(dirtest+'labels_d'+str(suffix+i+1)+'.parquet'))\n",
    "        testrecons.append(pd.read_parquet(dirtest+'recon2D_d'+str(suffix+i+1)+'.parquet'))\n",
    "testlabels_csv = pd.concat(testlabels, ignore_index=True)\n",
    "testrecons_csv = pd.concat(testrecons, ignore_index=True)\n",
    "\n",
    "iter_0, iter_1, iter_2 = 0, 0, 0\n",
    "iter_rem = 0\n",
    "for iter, row in testlabels_csv.iterrows():\n",
    "    if(abs(row['pt'])>threshold):\n",
    "        iter_0+=1\n",
    "    elif(-1*threshold<=row['pt']<0):\n",
    "        iter_1+=1\n",
    "    elif(0<row['pt']<=threshold):\n",
    "        iter_2+=1\n",
    "    else:\n",
    "        iter_rem+=1\n",
    "print(\"iter_0: \",iter_0)\n",
    "print(\"iter_1: \",iter_1)\n",
    "print(\"iter_2: \",iter_2)\n",
    "print(\"iter_rem: \",iter_rem)\n",
    "\n",
    "plt.hist(testlabels_csv['pt'], bins=100)\n",
    "plt.title('pT of all events')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(testlabels_csv[abs(testlabels_csv['pt'])>threshold]['pt'], bins=100)\n",
    "plt.title('pT of Class 0 events')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(testlabels_csv[(0<=testlabels_csv['pt'])&(testlabels_csv['pt']<=threshold)]['pt'], bins=50)\n",
    "plt.hist(testlabels_csv[(-1*threshold<=testlabels_csv['pt'])& (testlabels_csv['pt']<0)]['pt'], bins=50)\n",
    "plt.title('pT of Class 1+2 events')\n",
    "plt.show()\n",
    "\n",
    "number_of_events = (min(iter_1, iter_2)//1000)*1000\n",
    "if(number_of_events*2>iter_0):\n",
    "    number_of_events = (iter_0//1000)*1000/2\n",
    "number_of_events = int(number_of_events)\n",
    "print(\"Number of events: \",number_of_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "def sumRow(X):\n",
    "    sum1 = 0\n",
    "    sumList = []\n",
    "    for i in X:\n",
    "        sum1 = np.sum(i,axis=0)\n",
    "        sumList.append(sum1)\n",
    "        b = np.array(sumList)\n",
    "    return b\n",
    "testlist1, testlist2 = [], []\n",
    "\n",
    "for (index1, row1), (index2, row2) in zip(testrecons_csv.iterrows(), testlabels_csv.iterrows()):\n",
    "    rowSum = 0.0\n",
    "    X = row1.values\n",
    "    X = np.reshape(X,(13,21))\n",
    "    rowSum = sumRow(X)\n",
    "    testlist1.append(rowSum)\n",
    "    cls = -1\n",
    "    if(abs(row2['pt'])>threshold):\n",
    "        cls=0\n",
    "    elif(-1*threshold<=row2['pt']<0):\n",
    "        cls=1\n",
    "    elif(0<=row2['pt']<=threshold):\n",
    "        cls=2\n",
    "    testlist2.append([row2['y-local'], cls, row2['pt']])\n",
    "testdf_all = pd.concat([pd.DataFrame(testlist1), pd.DataFrame(testlist2 , columns=['y-local', 'cls', 'pt'])], axis=1)\n",
    "print(testdf_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalsize = number_of_events#227000\n",
    "random_seed0 = 10#11\n",
    "random_seed1 = 13#14\n",
    "random_seed2 = 19#20\n",
    "\n",
    "testdf_all = testdf_all.sample(frac=1, random_state=random_seed0).reset_index(drop=True)\n",
    "testdf_all.to_csv(dataset_savedir+'/'+'/FullTestData_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv', index=False)\n",
    "# testdfcls0 = testdf_all.loc[testdf_all['cls']==0]\n",
    "# testdfcls1 = testdf_all.loc[testdf_all['cls']==1]\n",
    "# testdfcls2 = testdf_all.loc[testdf_all['cls']==2]\n",
    "# print(testdfcls0.shape)\n",
    "# print(testdfcls1.shape)\n",
    "# print(testdfcls2.shape)\n",
    "# print(testdfcls2.head())\n",
    "# testdfcls0 = testdfcls0.iloc[:2*totalsize]\n",
    "# testdfcls1 = testdfcls1.iloc[:totalsize]\n",
    "# testdfcls2 = testdfcls2.iloc[:totalsize]\n",
    "# print(testdfcls2.head())\n",
    "\n",
    "# testcls0 = testdfcls0.sample(frac = 1, random_state=random_seed1)\n",
    "# testcls1 = testdfcls1.sample(frac = 1, random_state=random_seed1)\n",
    "# testcls2 = testdfcls2.sample(frac = 1, random_state=random_seed1)\n",
    "# test = pd.concat([testcls0, testcls1, testcls2], axis=0)\n",
    "\n",
    "# test = test.sample(frac=1, random_state=random_seed2)\n",
    "test=testdf_all\n",
    "# print(testcls0.shape)\n",
    "# print(testcls1.shape)\n",
    "# print(testcls2.shape)\n",
    "print(test.shape)\n",
    "\n",
    "testlabel = test['cls']\n",
    "testpt = test['pt']\n",
    "test = test.drop(['cls', 'pt'], axis=1)\n",
    "\n",
    "print(test.shape)\n",
    "print(testlabel.shape)\n",
    "print(testpt.shape)\n",
    "\n",
    "test.to_csv(dataset_savedir+'/FullPrecisionInputTestSet_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv', index=False)\n",
    "testlabel.to_csv(dataset_savedir+'/TestSetLabel_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv', index=False)\n",
    "testpt.to_csv(dataset_savedir+'/TestSetPt_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotdata = pd.concat([traincls0['pt'], testcls0['pt']], axis=0)\n",
    "# #print(plt.hist(traincls0['pt'], bins=100))\n",
    "# #print(plt.hist(testcls0['pt'], bins=100))\n",
    "# plt.hist(traincls0['pt'], bins=100)\n",
    "# plt.hist(testpt, bins=100)\n",
    "# plt.xticks(np.arange(-5, 5, 1))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2s\n",
    "# 0.1 threshold =  288000 total events for test/train!?\n",
    "# 0.125 thresh  = 432000 total events for test/train\n",
    "# 0.15 thresh   = 588000 total events for test/train!?\n",
    "# 0.175 thresh  = 748000 total events for test/train\n",
    "# 0.2 threshold =  908000 total events for test/train\n",
    "# 0.3 threshold = 1536000 total events for test/train!?\n",
    "# 0.4 threshold = 1998000 total events for test/train\n",
    "# 0.5 threshold = 1566000 total events for test/train\n",
    "# 100x25x100, dataset4s_50x12.5 follows above numbers. Not sure what about 50x25 which used the new random_seeds I believe.\n",
    "# For 100x25x150 and maybe for the new random seeds): 0,1=284000, 0.15=576000, 0.2=896000, 0.3=1516000, 0.4=1898000, 0.5=1550000\n",
    "\n",
    "# Dataset 1s\n",
    "# 0.1 threshold =  288000 total events for test/train\n",
    "# 0.15 threshold = 588000 total events for test/train\n",
    "# 0.2 threshold =  908000 total events for test/train\n",
    "# 0.3 threshold = 1536000 total events for test/train\n",
    "# 0.4 threshold = 1916000 total events for test/train\n",
    "# 0.5 threshold = 1484000 total events for test/train\n",
    "\n",
    "# Dataset 3s\n",
    "# 0.1 threshold =  52000 total events for train\n",
    "# 0.15 threshold = 96000 total events for train\n",
    "# 0.2 threshold =  136000 total events for train\n",
    "# 0.3 threshold =  216000 total events for train\n",
    "# 0.4 threshold =  296000 total events for train\n",
    "# 0.5 threshold =  376000 total events for train\n",
    "# for 100x25x150, the stats were lesser by 4k events all pt boundaries except 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
