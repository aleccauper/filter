{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed28315-565d-43b8-b48c-24300113360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TF_USE_LEGACY_KERAS 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets, svm, metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "# import keras\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e2528-8835-4499-8305-539b7978590a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sizes = ['50x12P5_0fb', '50x12P5_370fb', '50x12P5_1100fb', '100x25x150_0fb', '100x25x150_370fb', '100x25x150_1100fb']\n",
    "# sizes = ['50x15']\n",
    "# dataset_name = 'dataset_7s'\n",
    "# results_dir = 'results_7s'\n",
    "# models_dir = 'models_7s'\n",
    "dataset_name = 'dataset_9s_600NoiseThresh'\n",
    "results_dir = 'results_9s_600NoiseThresh'\n",
    "models_dir = 'models_9s_600NoiseThresh'\n",
    "thresholds = [0.1, 0.15, 0.2, 0.3, 0.4, 0.5]\n",
    "prime_num = [2,3,5,7,11,13,17,19,23,29]\n",
    "for run_iter in range(10):\n",
    "    for size_iter in sizes:\n",
    "        for threshold in thresholds:\n",
    "            tf.random.set_seed(prime_num[run_iter])\n",
    "            sensor_geom = size_iter\n",
    "            print(\"=============================\")\n",
    "            print(\"Run \"+str(run_iter)+\": Training model for \",sensor_geom,\" at pT boundary = \",threshold)\n",
    "            df1 = pd.read_csv('./'+dataset_name+'/FullPrecisionInputTrainSet_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of train dataset = \",df1.shape)\n",
    "            df2 = pd.read_csv('./'+dataset_name+'/TrainSetLabel_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of train-label set = \",df2.shape)\n",
    "            df3 = pd.read_csv('./'+dataset_name+'/FullPrecisionInputTestSet_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of test dataset = \",df3.shape)\n",
    "            df4 = pd.read_csv('./'+dataset_name+'/TestSetLabel_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of test-label set = \",df4.shape)\n",
    "            X_train = df1.values\n",
    "            X_test = df3.values\n",
    "            y_train = df2.values\n",
    "            y_test = df4.values\n",
    "            #X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state = 0)\n",
    "            print(\"X-train, X-test, Y-train, Y-test shapes = \",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "            X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "            model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Flatten(input_shape=(14,)),\n",
    "              tf.keras.layers.Dense(128, activation='relu'),\n",
    "              tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            model.compile(optimizer=Adam(),\n",
    "                          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), # default from_logits=False\n",
    "                          metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "            \n",
    "            model.summary()\n",
    "            \n",
    "            es = EarlyStopping(monitor='val_sparse_categorical_accuracy', \n",
    "                                               mode='max', # don't minimize the accuracy!\n",
    "                                               patience=20,\n",
    "                                               restore_best_weights=True)\n",
    "            \n",
    "            history = model.fit(X_train,\n",
    "                                y_train,\n",
    "                                callbacks=[es],\n",
    "                                epochs=200, \n",
    "                                batch_size=1024,\n",
    "                                validation_split=0.2,\n",
    "                                shuffle=True,\n",
    "                                verbose=1)\n",
    "            \n",
    "            history_dict = history.history\n",
    "            loss_values = history_dict['loss'] \n",
    "            val_loss_values = history_dict['val_loss'] \n",
    "            epochs = range(1, len(loss_values) + 1) \n",
    "            plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "            plt.plot(epochs, val_loss_values, 'orange', label='Validation loss')\n",
    "            plt.title('Training and validation loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.savefig('./'+results_dir+'/loss_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh_run'+str(run_iter)+'.png')\n",
    "            plt.close()\n",
    "            acc = history.history['sparse_categorical_accuracy']\n",
    "            val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "            epochs = range(1, len(acc) + 1)\n",
    "            plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "            plt.plot(epochs, val_acc, 'orange', label='Validation accuracy')\n",
    "            plt.title('Training and validation accuracy')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            #np.max(val_acc)\n",
    "            plt.savefig('./'+results_dir+'/accuracy_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh_run'+str(run_iter)+'.png')\n",
    "            plt.close()\n",
    "            preds = model.predict(X_test) \n",
    "            predictionsFiles =np.argmax(preds, axis=1)\n",
    "            pd.DataFrame(predictionsFiles).to_csv(\"./\"+results_dir+\"/predictionsFiles_\"+sensor_geom+\"_0P\"+str(threshold - int(threshold))[2:]+\"thresh_run\"+str(run_iter)+\".csv\",header='predict', index=False)\n",
    "            pd.DataFrame(y_test).to_csv(\"./\"+results_dir+\"/testResults_\"+sensor_geom+\"_0P\"+str(threshold - int(threshold))[2:]+\"thresh_run\"+str(run_iter)+\".csv\",header='true', index=False)\n",
    "            plt.hist(y_test, bins=30)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            score = model.evaluate(X_test, y_test, verbose=0)\n",
    "            print(\"Test loss:\", score[0])\n",
    "            print(\"Test accuracy:\", score[1])\n",
    "            \n",
    "            disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predictionsFiles)\n",
    "            disp.figure_.suptitle(\"Multiclassifier Confusion Matrix\")\n",
    "            print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "            plt.savefig('./'+results_dir+'/confusionMatrix_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter)+'.png')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            model.save_weights('./'+models_dir+'/trained_model_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter)+'.weights.h5')\n",
    "            model.save('./'+models_dir+'/trained_model_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter)+'.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ffbe5e-f8a6-4f1a-918b-f387da05104c",
   "metadata": {},
   "source": [
    "## Train on un-irradiated sensor and eval on irradited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6e8a5-0422-445f-a420-838237d5036a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sizes = ['50x12P5_0fb','50x12P5_370fb', '50x12P5_1100fb']\n",
    "dataset_name = 'dataset_9s_400NoiseThresh'\n",
    "results_dir = 'results_9s_400NoiseThresh_2s_trained'\n",
    "models_dir = 'models_9s_400NoiseThresh'\n",
    "thresholds = [0.1, 0.15, 0.2, 0.3, 0.4, 0.5]\n",
    "prime_num = [2,3,5,7,11,13,17,19,23,29]\n",
    "for run_iter in range(4,10):\n",
    "    for size_iter in sizes:\n",
    "        for threshold in thresholds:\n",
    "            tf.random.set_seed(prime_num[run_iter])\n",
    "            sensor_geom = size_iter\n",
    "            print(\"=============================\")\n",
    "            print(\"Run \"+str(run_iter)+\": Training model for \",sensor_geom,\" at pT boundary = \",threshold)\n",
    "            df1 = pd.read_csv('./'+dataset_name+'/FullPrecisionInputTrainSet_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of train dataset = \",df1.shape)\n",
    "            df2 = pd.read_csv('./'+dataset_name+'/TrainSetLabel_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of train-label set = \",df2.shape)\n",
    "            df3 = pd.read_csv('./'+dataset_name+'/FullPrecisionInputTestSet_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of test dataset = \",df3.shape)\n",
    "            df4 = pd.read_csv('./'+dataset_name+'/TestSetLabel_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of test-label set = \",df4.shape)\n",
    "            X_train = df1.values\n",
    "            X_test = df3.values\n",
    "            y_train = df2.values\n",
    "            y_test = df4.values\n",
    "            # X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state = 0)\n",
    "            print(\"X-test, Y-test shapes = \", X_test.shape, y_test.shape)\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "            X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "            \n",
    "            model = load_model('./'+models_dir+'/trained_model_50x12P5_0fb'+'_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter)+'.h5')\n",
    "            \n",
    "            model.summary()\n",
    "            \n",
    "            preds = model.predict(X_test) \n",
    "            predictionsFiles =np.argmax(preds, axis=1)\n",
    "            pd.DataFrame(predictionsFiles).to_csv(\"./\"+results_dir+\"/predictionsFiles_\"+sensor_geom+\"_0P\"+str(threshold - int(threshold))[2:]+\"thresh_run\"+str(run_iter)+\".csv\",header='predict', index=False)\n",
    "            pd.DataFrame(y_test).to_csv(\"./\"+results_dir+\"/testResults_\"+sensor_geom+\"_0P\"+str(threshold - int(threshold))[2:]+\"thresh_run\"+str(run_iter)+\".csv\",header='true', index=False)\n",
    "            # plt.hist(y_test, bins=30)\n",
    "            # plt.show()\n",
    "            # plt.close()\n",
    "            # score = model.evaluate(X_test, y_test, verbose=0)\n",
    "            # print(\"Test loss:\", score[0])\n",
    "            # print(\"Test accuracy:\", score[1])\n",
    "            # disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predictionsFiles)\n",
    "            # disp.figure_.suptitle(\"Multiclassifier Confusion Matrix\")\n",
    "            # print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "            # plt.savefig('./'+results_dir+'/confusionMatrix_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter)+'.png')\n",
    "            # plt.show()\n",
    "            # plt.close()\n",
    "            # # model.save_weights('./'+models_dir+'/trained_model_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter))\n",
    "            # # model.save('./'+models_dir+'/trained_model_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc792e-0368-44fb-a60c-e232ebfe6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trained models from \",sensor_geom[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902fd76-edd0-4d41-bd04-0de8153819c0",
   "metadata": {},
   "source": [
    "## uncentered-data evaluated using model trained on centered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214bc0b-9746-4be7-ae52-ebb93edd92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = ['50x12P5']\n",
    "dataset_name = 'dataset_3su'\n",
    "results_dir = 'results_3su'\n",
    "models_dir = 'models_3s'\n",
    "# sizes = ['50x12P5_0fb','50x12P5_370fb', '50x12P5_1100fb']\n",
    "# dataset_name = 'dataset_9s_400NoiseThresh'\n",
    "# results_dir = 'results_9s_400NoiseThresh_2s_trained'\n",
    "# models_dir = 'models_9s_400NoiseThresh'\n",
    "thresholds = [0.1, 0.15, 0.2, 0.3, 0.4, 0.5]\n",
    "prime_num = [2,3,5,7,11,13,17,19,23,29]\n",
    "for run_iter in range(10):\n",
    "    for size_iter in sizes:\n",
    "        for threshold in thresholds:\n",
    "            tf.random.set_seed(prime_num[run_iter])\n",
    "            sensor_geom = size_iter\n",
    "            print(\"=============================\")\n",
    "            print(\"Run \"+str(run_iter)+\": Training model for \",sensor_geom,\" at pT boundary = \",threshold)\n",
    "            df1 = pd.read_csv('./'+dataset_name+'/FullPrecisionInputTrainSet_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of train dataset = \",df1.shape)\n",
    "            df2 = pd.read_csv('./'+dataset_name+'/TrainSetLabel_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of train-label set = \",df2.shape)\n",
    "            df3 = pd.read_csv('./'+dataset_name+'/FullPrecisionInputTestSet_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of test dataset = \",df3.shape)\n",
    "            df4 = pd.read_csv('./'+dataset_name+'/TestSetLabel_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'thresh.csv')\n",
    "            print(\"Shape of test-label set = \",df4.shape)\n",
    "            X_train = df1.values\n",
    "            X_test = df3.values\n",
    "            y_train = df2.values\n",
    "            y_test = df4.values\n",
    "            # X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state = 0)\n",
    "            print(\"X-test, Y-test shapes = \", X_test.shape, y_test.shape)\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "            X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "            model = load_model('./'+models_dir+'/trained_model_50x12P5_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter)+'.h5')\n",
    "            \n",
    "            model.summary()\n",
    "            \n",
    "            preds = model.predict(X_test) \n",
    "            predictionsFiles =np.argmax(preds, axis=1)\n",
    "            pd.DataFrame(predictionsFiles).to_csv(\"./\"+results_dir+\"/predictionsFiles_\"+sensor_geom+\"_0P\"+str(threshold - int(threshold))[2:]+\"thresh_run\"+str(run_iter)+\".csv\",header='predict', index=False)\n",
    "            pd.DataFrame(y_test).to_csv(\"./\"+results_dir+\"/testResults_\"+sensor_geom+\"_0P\"+str(threshold - int(threshold))[2:]+\"thresh_run\"+str(run_iter)+\".csv\",header='true', index=False)\n",
    "            plt.hist(y_test, bins=30)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            score = model.evaluate(X_test, y_test, verbose=0)\n",
    "            print(\"Test loss:\", score[0])\n",
    "            print(\"Test accuracy:\", score[1])\n",
    "            disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predictionsFiles)\n",
    "            disp.figure_.suptitle(\"Multiclassifier Confusion Matrix\")\n",
    "            print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "            plt.savefig('./'+results_dir+'/confusionMatrix_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter)+'.png')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            # model.save_weights('./'+models_dir+'/trained_model_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter))\n",
    "            # model.save('./'+models_dir+'/trained_model_'+sensor_geom+'_0P'+str(threshold - int(threshold))[2:]+'_run'+str(run_iter)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a77e1-6bed-43b5-9868-8f30de20ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5582a9-42e8-41e4-a19e-12fb6c59b4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
